{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "from torchvision import datasets, transforms, models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:/Windows/System32/kwontest2/data'\n",
    "\n",
    "# TODO : Resize the datasets\n",
    "data_transform = transforms.Compose([transforms.Resize((160, 160))])\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "dataset = datasets.ImageFolder(data_dir, transform = data_transform)\n",
    "\n",
    "# TODO: Get names of peoples from folder names\n",
    "idx_to_class = {i:c for c,i in dataset.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing MTCNN and InceptionResnetV1 \n",
    "pic1 = MTCNN(image_size=160, margin=0, keep_all=False, min_face_size=40)\n",
    "pic2 = MTCNN(image_size=160, margin=0, keep_all=True, min_face_size=40)\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn)\n",
    "\n",
    "face_list = [] # list of cropped faces from photos folder\n",
    "name_list = [] # list of names corrospoing to cropped photos\n",
    "embedding_list = [] # list of embeding matrix after conversion from cropped faces to embedding matrix using resnet\n",
    "\n",
    "for img, idx in loader:\n",
    "    face, prob = pic2(img, return_prob=True) \n",
    "    if face is not None and prob>0.90: # if face detected and porbability > 90%\n",
    "        emb = model(face) # passing cropped face into resnet model to get embedding matrix\n",
    "        embedding_list.append(emb.detach()) # resulten embedding matrix is stored in a list\n",
    "        name_list.append(idx_to_class[idx]) # names are stored in a list\n",
    "        \n",
    "# save data\n",
    "data = [face_list, embedding_list, name_list] \n",
    "torch.save(data, 'C:/Windows/System32/kwontest2/data.pt') # saving data.pt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2bce8d737c35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m                     \u001b[0mdist_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                 \u001b[0mmin_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# get minumum dist value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m                 \u001b[0mmin_dist_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_dist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# get minumum dist index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmin_dist_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# get name corrosponding to minimum dist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "# Using webcam recognize face\n",
    "\n",
    "# loading data.pt file\n",
    "load_data = torch.load('data.pt') \n",
    "embedding_list = load_data[0] \n",
    "name_list = load_data[1] \n",
    "\n",
    "cam = cv2.VideoCapture(0) \n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"fail to grab frame, try again\")\n",
    "        break\n",
    "        \n",
    "    img = Image.fromarray(frame)\n",
    "    img_cropped_list, prob_list = pic2(img, return_prob=True) \n",
    "    \n",
    "    if img_cropped_list is not None:\n",
    "        boxes, _ = pic2.detect(img)\n",
    "                \n",
    "        for i, prob in enumerate(prob_list):\n",
    "            if prob>0.90:\n",
    "                emb = model(img_cropped_list[i].unsqueeze(0)).detach() \n",
    "                \n",
    "                dist_list = [] # list of matched distances, minimum distance is used to identify the person\n",
    "                \n",
    "                for idx, emb_db in enumerate(embedding_list):\n",
    "                    dist = torch.dist(emb, emb_db).item()\n",
    "                    dist_list.append(dist)\n",
    "\n",
    "                min_dist = min(dist_list) # get minumum dist value\n",
    "                min_dist_idx = dist_list.index(min_dist) # get minumum dist index\n",
    "                name = name_list[min_dist_idx] # get name corrosponding to minimum dist\n",
    "                \n",
    "                box = boxes[i] \n",
    "                \n",
    "                original_frame = frame.copy() # storing copy of frame before drawing on it\n",
    "                \n",
    "                if min_dist<0.90:\n",
    "                    frame = cv2.putText(frame, name+' '+str(min_dist), (box[0],box[1]), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),1, cv2.LINE_AA)\n",
    "                \n",
    "                frame = cv2.rectangle(frame, (box[0],box[1]) , (box[2],box[3]), (255,0,0), 2)\n",
    "\n",
    "    cv2.imshow(\"IMG\", frame)\n",
    "        \n",
    "    \n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256==27: # ESC\n",
    "        print('Esc pressed, closing...')\n",
    "        break\n",
    "        \n",
    "    elif k%256==32: # space to save image\n",
    "        print('Enter your name :')\n",
    "        name = input()\n",
    "        \n",
    "        # create directory if not exists\n",
    "        if not os.path.exists('photos/'+name):\n",
    "            os.mkdir('photos/'+name)\n",
    "            \n",
    "        img_name = \"photos/{}/{}.jpg\".format(name, int(time.time()))\n",
    "        cv2.imwrite(img_name, original_frame)\n",
    "        print(\" saved: {}\".format(img_name))\n",
    "        \n",
    "        \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
